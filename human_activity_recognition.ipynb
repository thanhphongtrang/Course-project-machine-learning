{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Supervised Learning Model: Human Activity Recognition\n",
    "\n",
    "**Author:** Phong Trang Tran Thanh (Python version)\n",
    "**Date:** Mar 7, 2025\n",
    "\n",
    "## Overview\n",
    "\n",
    "### Data Description\n",
    "This project examines the Weight Lifting Exercise Data provided by Velloso et al. (2013). They used wearable devices to measure the acceleration of 6 participants to see if the participants were doing the exercises correctly. The accelerometers on the belt, forearm, arm, and dumbbell of 6 participants provide information on the exercise movements. There are 5 classes of the labels: A, B, C, D, and E; only class A is the correct movement while other classes correspond to common mistakes of the training.\n",
    "\n",
    "* **Training Data:** [pml-training.csv](https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv)\n",
    "* **Testing Data:** [pml-testing.csv](https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv)\n",
    "\n",
    "### The Project Goal\n",
    "This project aims to use data provided by the accelerators to build a supervised learning model that predicts the outcome of correct/incorrect training movements. Two models will be built and evaluated:\n",
    "1.  Random Forest model\n",
    "2.  Support Vector Machine (SVM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, classification_report\n",
    "\n",
    "# Set plot style\n",
    "sns.set_theme(style=\"whitegrid\")\n",
    "plt.rcParams['figure.figsize'] = (10, 7.25)\n",
    "\n",
    "print(\"Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Loading and Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# URLs for the datasets\n",
    "url_train = \"https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv\"\n",
    "url_test = \"https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv\"\n",
    "\n",
    "# Load the datasets using pandas\n",
    "# The na_values parameter handles strings that should be treated as NaN\n",
    "training_df = pd.read_csv(url_train, na_values=[\"\", \"NA\", \"#DIV/0!\"])\n",
    "testing_df = pd.read_csv(url_test, na_values=[\"\", \"NA\", \"#DIV/0!\"])\n",
    "\n",
    "print(\"Dimension of the training dataset:\", training_df.shape)\n",
    "print(\"Dimension of the testing dataset:\", testing_df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initial Data Exploration\n",
    "Let's check the distribution of the target variable `classe`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 6))\n",
    "sns.countplot(x='classe', data=training_df, palette=\"viridis\", order=sorted(training_df['classe'].unique()))\n",
    "plt.title('Number of Observations in Each Class')\n",
    "plt.xlabel('Exercise Class')\n",
    "plt.ylabel('Count')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset is reasonably balanced, which is good for model training.\n",
    "\n",
    "### Split Data into Training and Validation Sets\n",
    "We will split the original training data into a new training set (80%) and a validation set (20%) to evaluate our models. We use `stratify` to ensure the class distribution is the same in both sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate features (X) and target (y)\n",
    "X = training_df.drop('classe', axis=1)\n",
    "y = training_df['classe']\n",
    "\n",
    "# Split the data\n",
    "X_train, X_validation, y_train, y_validation = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=123, stratify=y\n",
    ")\n",
    "\n",
    "print(\"Training set dimensions:\", X_train.shape)\n",
    "print(\"Validation set dimensions:\", X_validation.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Feature Selection for Modeling\n",
    "\n",
    "We apply a multi-step feature selection process to the training data. The same transformations will be applied to the validation and test sets.\n",
    "\n",
    "### Step 1: Remove columns with a high percentage of NA values\n",
    "First, we remove any column that contains more than 50% NA values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the percentage of missing values for each column\n",
    "missing_percentage = X_train.isnull().sum()
